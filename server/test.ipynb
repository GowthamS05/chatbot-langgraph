{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, Optional, Dict, Any, List, Union\n",
    "from langgraph.graph import add_messages, StateGraph, END\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from uuid import uuid4\n",
    "import json\n",
    "import pandas as pd\n",
    "import asyncio\n",
    "from langchain_core.messages import AIMessage, HumanMessage, ToolMessage, SystemMessage\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field \n",
    "from typing import Annotated, Sequence, List, Literal \n",
    "from langgraph.types import Command \n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize LLM\n",
    "model= ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-001\", temperature=0)\n",
    "search_tool = TavilySearchResults(max_results=1)\n",
    "tools = [search_tool]\n",
    "llm_with_tools = model.bind_tools(tools)\n",
    "\n",
    "# Set up memory and tools\n",
    "memory = MemorySaver()\n",
    "\n",
    "# Create a mock JSON dataset for demonstration\n",
    "MOCK_JSON = \"\"\"\n",
    "[\n",
    "    {\"date\": \"2023-01-01\", \"sales\": 1200, \"region\": \"North\", \"product\": \"Widget A\"},\n",
    "    {\"date\": \"2023-01-01\", \"sales\": 950, \"region\": \"South\", \"product\": \"Widget A\"},\n",
    "    {\"date\": \"2023-01-01\", \"sales\": 1500, \"region\": \"East\", \"product\": \"Widget B\"},\n",
    "    {\"date\": \"2023-01-01\", \"sales\": 1100, \"region\": \"West\", \"product\": \"Widget B\"},\n",
    "    {\"date\": \"2023-01-02\", \"sales\": 1300, \"region\": \"North\", \"product\": \"Widget A\"},\n",
    "    {\"date\": \"2023-01-02\", \"sales\": 1000, \"region\": \"South\", \"product\": \"Widget A\"},\n",
    "    {\"date\": \"2023-01-02\", \"sales\": 1600, \"region\": \"East\", \"product\": \"Widget B\"},\n",
    "    {\"date\": \"2023-01-02\", \"sales\": 1050, \"region\": \"West\", \"product\": \"Widget B\"}\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "# Define the state type\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    request_type: str\n",
    "    formula: Optional[str]\n",
    "    dataframe: Optional[Dict[str, Any]]\n",
    "\n",
    "# Supervisor model definition\n",
    "class Supervisor(BaseModel):\n",
    "    next: Literal[\"summary\", \"open_prompt\", \"model\"] = Field(\n",
    "        description=\"Determines which specialist to activate next in the workflow sequence: \"\n",
    "                    \"'summary' When user ask question regarding summary of the table. \"\n",
    "                    \"'open_prompt' When he asks question about data table. Eg) What is my highest or lowest \"\n",
    "                    \"'model' When user ask about current happening or some thing that can be searched in google \"\n",
    "    )\n",
    "    reason: str = Field(\n",
    "        description=\"Detailed justification for the routing decision, explaining the rationale behind selecting the particular specialist and how this advances the task toward completion.\"\n",
    "    )\n",
    "\n",
    "# Node definitions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def supervisor(state: State) -> Command:\n",
    "    system_prompt = '''\n",
    "Act like a supervisor agent in a multi-agent system. Your job is to decide which specialist to activate based on the user's question.\n",
    "\n",
    "Return only one of the following strings:\n",
    "- \"summary\" → when the user asks for a summary of a table.\n",
    "- \"open_prompt\" → when the user asks specific questions about data, e.g., highest or lowest values.\n",
    "- \"model\" → when the user asks about current events or things you'd find on Google.\n",
    "\n",
    "Only return one of the four strings. Do not explain. Do not respond to the user.\n",
    "\n",
    "Take a deep breath and work on this problem step-by-step.\n",
    "'''\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},  \n",
    "    ] + state[\"messages\"] \n",
    "\n",
    "    response = model.with_structured_output(Supervisor).invoke(messages)\n",
    "\n",
    "    goto = response.next\n",
    "    reason = response.reason\n",
    "    \n",
    "    return Command(    \n",
    "        update={\n",
    "            \"request_type\": goto,\n",
    "            \"messages\": [\n",
    "                HumanMessage(content=reason, name=\"supervisor\")\n",
    "            ]\n",
    "        },\n",
    "        goto=goto,  \n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def open_prompt(state: State) -> Dict:\n",
    "    \"\"\"\n",
    "    Converts user query to a pandas DataFrame formula.\n",
    "    \"\"\"\n",
    "    # Get the last human message\n",
    "    human_messages = [msg for msg in state[\"messages\"] if isinstance(msg, HumanMessage)]\n",
    "    last_human_message = human_messages[-1].content\n",
    "    \n",
    "    # Create a prompt for the LLM to generate a pandas formula\n",
    "    prompt = f\"\"\"\n",
    "    Convert the following user query into a pandas DataFrame formula.\n",
    "    Use 'df' as the variable name for the DataFrame.\n",
    "    \n",
    "    User query: {last_human_message}\n",
    "    \n",
    "    Example:\n",
    "    If the query is \"Find sales above 1200 in the East region\", the formula would be:\n",
    "    df[(df['sales'] > 1200) & (df['region'] == 'East')]\n",
    "    \n",
    "    Only return the pandas code, nothing else.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate formula using LLM\n",
    "    formula_result = await model.ainvoke([HumanMessage(content=prompt)])\n",
    "    formula = formula_result.content.strip()\n",
    "    \n",
    "    # Clean up the formula (remove backticks if present)\n",
    "    if formula.startswith(\"```python\"):\n",
    "        formula = formula.split(\"```python\")[1]\n",
    "    if formula.endswith(\"```\"):\n",
    "        formula = formula.split(\"```\")[0]\n",
    "    formula = formula.strip()\n",
    "    \n",
    "    # Update state with the formula\n",
    "    return {\n",
    "        \"formula\": formula,\n",
    "        \"messages\": state[\"messages\"]  # Keep existing messages\n",
    "    }\n",
    "\n",
    "async def open_prompt_validator(state: State) -> Union[Dict, str]:\n",
    "    \"\"\"\n",
    "    Validates the formula by running it against the DataFrame.\n",
    "    Uses the REPL tool to execute the formula and check the results.\n",
    "    \"\"\"\n",
    "    # Get the formula from state\n",
    "    formula = state.get(\"formula\")\n",
    "    if not formula:\n",
    "        return {\"messages\": [AIMessage(content=\"No formula found to validate.\")]}\n",
    "    \n",
    "    \n",
    "\n",
    "    # This would normally use the actual REPL tool, but here we'll simulate it\n",
    "    try:\n",
    "        # In a real implementation, you would use something like:\n",
    "        # repl_result = await repl_tool.ainvoke({\"code\": code})\n",
    "        \n",
    "        # For demonstration, let's simulate REPL execution\n",
    "        # Parse mock JSON data\n",
    "        data = json.loads(MOCK_JSON)\n",
    "        df = pd.DataFrame(data)\n",
    "        \n",
    "        # Safely evaluate the formula (note: this is simplified for demo)\n",
    "        # In production, use a proper REPL or sanitized eval\n",
    "        local_vars = {\"df\": df}\n",
    "        try:\n",
    "            # Very unsafe in production, use proper repl tool instead\n",
    "            result_df = eval(formula, {\"__builtins__\": {}}, local_vars)\n",
    "            if not isinstance(result_df, pd.DataFrame):\n",
    "                if isinstance(result_df, pd.Series):\n",
    "                    result_df = result_df.to_frame()\n",
    "                else:\n",
    "                    result_df = pd.DataFrame()\n",
    "            \n",
    "            row_count = len(result_df)\n",
    "            \n",
    "            if row_count > 0:\n",
    "                result_preview = result_df.head(10).to_dict(orient='records')\n",
    "                result_message = f\"Preview: {json.dumps(result_preview, indent=2)}\"\n",
    "                prompt = f\"\"\"\n",
    "                    Act as a data expert analyst ,\n",
    "                    Please summarize the following data :\n",
    "                    {result_message}\n",
    "                    Provide key insights about the data.Make sure its consice.\n",
    "                    \"\"\"\n",
    "        \n",
    "                summary_result = await model.ainvoke([HumanMessage(content=prompt)])\n",
    "                return {\n",
    "                    \"messages\": [AIMessage(content=summary_result.content)],\n",
    "                    \"request_type\": \"open_prompt\"\n",
    "\n",
    "                }\n",
    "          \n",
    "            else:\n",
    "                # No rows found, go back to formula node\n",
    "                return {\n",
    "                    \"messages\": [AIMessage(content=\"Please try again!!!!\")],\n",
    "                    \"request_type\": \"open_prompt\"\n",
    "\n",
    "                }\n",
    "                \n",
    "        except Exception as e:\n",
    "            # Formula error, go back to formula node\n",
    "            error_message = f\"Error in formula: {str(e)}. Let me try a different approach.\"\n",
    "            return {\n",
    "                \"messages\": [AIMessage(content=error_message)],\n",
    "                \"formula\": None  # Clear the invalid formula\n",
    "            }\n",
    "            \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"messages\": [AIMessage(content=f\"Validation error: {str(e)}\")],\n",
    "            \"formula\": None  # Clear the formula on error\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def summary(state: State) -> Dict:\n",
    "    \"\"\"\n",
    "    Converts mock JSON to DataFrame, calculates summary statistics,\n",
    "    and updates the state with this information.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = json.loads(MOCK_JSON)\n",
    "        df = pd.DataFrame(data)\n",
    "        \n",
    "        # Calculate summary statistics for numeric columns\n",
    "        numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "        summary_data = {}\n",
    "        \n",
    "        for col in numeric_cols:\n",
    "            summary_data[col] = {\n",
    "                \"min\": df[col].min(),\n",
    "                \"max\": df[col].max(),\n",
    "                \"avg\": df[col].mean()\n",
    "            }\n",
    "        \n",
    "        # Prepare data for LLM summarization\n",
    "        prompt = f\"\"\"\n",
    "        Please summarize the following data statistics:\n",
    "        {summary_data}\n",
    "        Provide key insights about the data . Make sure its consice.\n",
    "        \"\"\"\n",
    "        \n",
    "        summary_result = await model.ainvoke([HumanMessage(content=prompt)])\n",
    "        \n",
    "        # Return updated state with summary data\n",
    "        return {\n",
    "            \"messages\": [AIMessage(content=summary_result.content)],\n",
    "            \"request_type\": \"summary_chat\"\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error in summary node: {str(e)}\"\n",
    "        return {\"messages\": [AIMessage(content=error_msg)]}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def model_node(state: State) -> Dict:\n",
    "    \"\"\"LLM node that processes messages.\"\"\"\n",
    "    result = await llm_with_tools.ainvoke(state[\"messages\"])\n",
    "    return {\n",
    "        \"messages\": [result],\n",
    "        \"request_type\": \"model_response\"\n",
    "    }\n",
    "\n",
    "async def tools_router(state: State) -> str:\n",
    "    \"\"\"Routes to tool_node if the last message has tool calls, otherwise ends.\"\"\"\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if hasattr(last_message, \"tool_calls\") and len(last_message.tool_calls) > 0:\n",
    "        return \"tool_node\"\n",
    "    else: \n",
    "        return END\n",
    "\n",
    "async def tool_node(state: State) -> Dict:\n",
    "    \"\"\"Custom tool node that handles tool calls from the LLM.\"\"\"\n",
    "    # Get the tool calls from the last message\n",
    "    tool_calls = state[\"messages\"][-1].tool_calls\n",
    "    \n",
    "    # Initialize list to store tool messages\n",
    "    tool_messages = []\n",
    "    \n",
    "    # Process each tool call\n",
    "    for tool_call in tool_calls:\n",
    "        tool_name = tool_call[\"name\"]\n",
    "        tool_args = tool_call[\"args\"]\n",
    "        tool_id = tool_call[\"id\"]\n",
    "        \n",
    "        # Handle the search tool\n",
    "        if tool_name == \"tavily_search_results_json\":\n",
    "            # Execute the search tool with the provided arguments\n",
    "            search_results = await search_tool.ainvoke(tool_args)\n",
    "            \n",
    "            # Create a ToolMessage for this result\n",
    "            tool_message = ToolMessage(\n",
    "                content=str(search_results),\n",
    "                tool_call_id=tool_id,\n",
    "                name=tool_name\n",
    "            )\n",
    "            \n",
    "            tool_messages.append(tool_message)\n",
    "    \n",
    "    # Add the tool messages to the state\n",
    "    return {\"messages\": tool_messages}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph():\n",
    "    # Initialize the graph\n",
    "    graph_builder = StateGraph(State)\n",
    "    \n",
    "    # Add nodes\n",
    "    graph_builder.add_node(\"supervisor\", supervisor)\n",
    "    graph_builder.add_node(\"summary\", summary)\n",
    "    graph_builder.add_node(\"model\", model_node)\n",
    "    graph_builder.add_node(\"open_prompt\",open_prompt)\n",
    "    graph_builder.add_node(\"open_prompt_validator\",open_prompt_validator)\n",
    "    graph_builder.add_node(\"tool_node\", tool_node)\n",
    "\n",
    "    # Set entry point\n",
    "    graph_builder.set_entry_point(\"supervisor\")\n",
    "    \n",
    "    # Add edges\n",
    "    graph_builder.add_edge(\"summary\", END)\n",
    "    \n",
    "    # Conditional edges for tool handling\n",
    "    graph_builder.add_conditional_edges(\"model\", tools_router)\n",
    "    graph_builder.add_edge(\"tool_node\", \"model\")\n",
    "    graph_builder.add_edge(\"open_prompt\",\"open_prompt_validator\")\n",
    "    graph_builder.add_edge(\"open_prompt_validator\", END)\n",
    "\n",
    "    \n",
    "    # Compile the graph\n",
    "    return graph_builder.compile()\n",
    "\n",
    "# Main execution function\n",
    "graph = build_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='What has the both lowest and highest sales', additional_kwargs={}, response_metadata={}, id='88a8567f-34d1-4aac-ac88-c1ca6524c100'),\n",
       "  HumanMessage(content=\"The user is asking a specific question about the data, specifically about the lowest and highest sales, which requires analyzing the data table. Therefore, the 'open_prompt' specialist is the most appropriate choice to handle this type of query as it involves data analysis and retrieval of specific values from the table.\", additional_kwargs={}, response_metadata={}, name='supervisor', id='c1c8c06a-c1ab-4a17-bc5e-25158ab29f0d'),\n",
       "  AIMessage(content=\"Okay, here's a concise summary and key insights based on the provided data:\\n\\n**Summary:**\\n\\nThe data represents sales figures from two data points. The first data point has sales of 950, and the second has sales of 1600.\\n\\n**Key Insights:**\\n\\n*   **Sales Growth:** There is a significant increase in sales between the two data points (from 950 to 1600).\\n*   **Limited Context:** Without more information (e.g., time period, product, region), it's difficult to draw deeper conclusions. We can only observe the difference in sales between the two points.\", additional_kwargs={}, response_metadata={}, id='ae9878a1-e420-488f-af75-695cf0ca4e55')],\n",
       " 'request_type': 'open_prompt',\n",
       " 'formula': \"df[['sales']].agg(['min', 'max'])\"}"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\n",
    "    \"configurable\": {\n",
    "        \"thread_id\": 5\n",
    "    }\n",
    "}\n",
    "\n",
    "response = await graph.ainvoke({\n",
    "    \"messages\": [HumanMessage(content=\"What has the both lowest and highest sales\")], \n",
    "})\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, here's a concise summary and key insights from the provided sales data:\n",
      "\n",
      "**Summary:**\n",
      "\n",
      "The data represents sales figures for two products, Widget A and Widget B, across four regions (North, South, East, and West) over a two-day period (2023-01-01 and 2023-01-02).\n",
      "\n",
      "**Key Insights:**\n",
      "\n",
      "*   **Product Performance:** Widget B generally outperforms Widget A in terms of sales volume.\n",
      "*   **Regional Performance:** The East region shows the highest sales figures, primarily driven by Widget B. The South region has the lowest sales.\n",
      "*   **Temporal Trend:** Sales appear relatively stable across the two days, with slight increases in some regions and products.\n",
      "*   **Regional Product Preference:** Widget A is more popular in the North and South regions, while Widget B dominates the East and West regions.\n",
      "\n",
      "**In essence, Widget B in the East region is the top performer, while Widget A in the South region is the weakest.**\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"configurable\": {\n",
    "        \"thread_id\": 8\n",
    "    }\n",
    "}\n",
    "\n",
    "# Use async for to iterate over the async generator\n",
    "events=graph.astream_events({\n",
    "    \"messages\": [HumanMessage(content=\"the value\")],\n",
    "}, version=\"v2\")\n",
    "\n",
    "\n",
    "result=\"\"\n",
    "\n",
    "async for event in events: \n",
    "    if event[\"event\"] == \"on_chat_model_stream\":\n",
    "        if (\n",
    "    isinstance(event, dict)\n",
    "    and 'metadata' in event\n",
    "    and isinstance(event['metadata'], dict)\n",
    "    and event['metadata'].get('langgraph_node') not in ('supervisor', 'open_prompt')):\n",
    "\n",
    "            print(event[\"data\"][\"chunk\"].content, end=\"\", flush=True)\n",
    "            result = result + event[\"data\"][\"chunk\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Okay, here's a concise summary and key insights from the provided sales data:\n",
       "\n",
       "**Summary:**\n",
       "\n",
       "The data represents sales figures for two products, Widget A and Widget B, across four regions (North, South, East, and West) over a two-day period (2023-01-01 and 2023-01-02).\n",
       "\n",
       "**Key Insights:**\n",
       "\n",
       "*   **Product Performance:** Widget B generally outperforms Widget A in terms of sales volume.\n",
       "*   **Regional Performance:** The East region shows the highest sales figures, primarily driven by Widget B. The South region has the lowest sales.\n",
       "*   **Temporal Trend:** Sales appear relatively stable across the two days, with slight increases in some regions and products.\n",
       "*   **Regional Product Preference:** Widget A is more popular in the North and South regions, while Widget B dominates the East and West regions.\n",
       "\n",
       "**In essence, Widget B in the East region is the top performer, while Widget A in the South region is the weakest.**\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown, Latex\n",
    "Markdown(result)\n",
    "# print(event[\"data\"][\"chunk\"].content, end=\"\", flush=True)\n",
    "#             result = result + event[\"data\"][\"chunk\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
